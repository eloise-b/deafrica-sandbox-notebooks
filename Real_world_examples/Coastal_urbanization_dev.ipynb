{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coastal Urbanization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Problem:</b> There is an issue with this notebook/class where dask seems to start failing before the displayed memory reaches the instance memory limit. I have tried changing some of the dask settings, persisting vs computing, chunks size etc. I would like the intermdiate arrays for other parts of the analysis. The steps seem to work when done manually outside of the class. Hope its a simple error and easy to solve. Thanks!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "import sys\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from datacube.utils.cog import write_cog\n",
    "import folium.plugins\n",
    "import folium\n",
    "from odc.algo import xr_geomedian\n",
    "from odc.ui import with_ui_cbk\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_datahandling import load_ard\n",
    "from deafrica_datahandling import mostcommon_crs\n",
    "from deafrica_bandindices import calculate_indices\n",
    "from deafrica_coastaltools import tidal_tag\n",
    "from deafrica_spatialtools import subpixel_contours\n",
    "from deafrica_spatialtools import interpolate_2d\n",
    "from deafrica_spatialtools import contours_to_arrays\n",
    "from deafrica_plotting import display_map\n",
    "from deafrica_plotting import map_shapefile\n",
    "from deafrica_plotting import rgb\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "import rioxarray\n",
    "import rasterio\n",
    "import rasterio.windows as windows\n",
    "from rasterio.enums import Resampling\n",
    "from skimage.morphology import disk, dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='coastal_urbanization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_local_dask_cluster(n_workers=15, spare_mem= '1gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "#possible full analysis class?\n",
    "#TODO upgrade to using named tuples but would have to change parameters\n",
    "class Location(NamedTuple):\n",
    "    name: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    area: float = .10\n",
    "        \n",
    "stl = Location(\n",
    "    name = 'stlouis',\n",
    "    lat = 16.01366,\n",
    "    lon = -16.49195\n",
    ")\n",
    "\n",
    "    \n",
    "saly = Location(\n",
    "    name = 'saly',\n",
    "    lat = 14.43,\n",
    "    lon = -17.00\n",
    ")\n",
    "\n",
    "corniche = Location(\n",
    "    name = 'corniche',\n",
    "    lat = 14.72618,\n",
    "    lon = -17.50387 \n",
    ")\n",
    "\n",
    "locations =  [saly, corniche, stl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    \n",
    "    def __init__(self, loc: Location, dc: datacube):\n",
    "        self.datacube = dc\n",
    "        self.name= loc.name\n",
    "        self.location = (loc.lat, loc.lon) #TODO change name to \"coordinates\" or Location class\n",
    "        self.latitude = self.lat = loc.lat\n",
    "        self.longitude = self.lon = loc.lon\n",
    "        self.create_extent()\n",
    "        self.create_ds()\n",
    "        self.create_mosaic()\n",
    "        self.create_indices()\n",
    "        self.path =  f'./urbanisation/{self.name}'\n",
    "        self.map = display_map(**self.extent)\n",
    "    \n",
    "    def create_extent(self, area = .10):\n",
    "        #transforms a location into area for loading\n",
    "        buffer= area/2\n",
    "        lat_range = (self.lat-buffer, self.lat+buffer)\n",
    "        lon_range = (self.lon-buffer, self.lon+buffer)\n",
    "        self.extent = { 'y': lat_range, 'x': lon_range}\n",
    "        self.upper_left = self.ul = [self.extent['y'][0], self.extent['x'][0]]\n",
    "        self.upper_right = self.ur = [self.extent['y'][0], self.extent['x'][1]]\n",
    "        self.lower_right = self.lr = [self.extent['y'][1], self.extent['x'][1]]\n",
    "        self.lower_left = self.ll = [self.extent['y'][1], self.extent['x'][0]]\n",
    "    \n",
    "    def create_ds(self, query = None):\n",
    "\n",
    "        #Defaul query\n",
    "        if query == None:\n",
    "\n",
    "            time_range = ('2000', '2018')\n",
    "\n",
    "            products=['s2_l2a']\n",
    "\n",
    "                    #[  'ls5_usgs_sr_scene', \n",
    "                    #   'ls7_usgs_sr_scene', \n",
    "                    #   'ls8_usgs_sr_scene']\n",
    "\n",
    "            bands = ['red',\n",
    "                     'green',\n",
    "                     'blue', \n",
    "                     'nir',\n",
    "                     'swir_1',\n",
    "                     'swir_2']\n",
    "\n",
    "            query = {\n",
    "                'time': time_range,\n",
    "                'products': products,\n",
    "                'measurements': bands,\n",
    "                'resolution': (-10, 10),\n",
    "                'output_crs': 'EPSG:6933',\n",
    "                'min_gooddata': 0\n",
    "            }\n",
    "\n",
    "        #Load the data from the datacube\n",
    "        self.dataset = load_ard(dc=self.datacube,\n",
    "                      #align=(15, 15),\n",
    "                      group_by='solar_day',\n",
    "                      dask_chunks={'time' : -1, 'x' : 500, 'y' : 500}, #was messing the chunk size\n",
    "                      #progress_cbk=with_ui_cbk(),\n",
    "                      **self.extent,\n",
    "                      **query)\n",
    "\n",
    "        #sanity check on values\n",
    "        #ds = ds.where(0 <= ds).where(ds <= 10000)\n",
    "    \n",
    "    def set_dataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def set_mosaic(self, mosaic):\n",
    "        self.mosaic = mosaic\n",
    "        \n",
    "    def compute(self):\n",
    "        self.dataset = self.dataset.persist()\n",
    "        self.mosaic = self.mosaic.persist()\n",
    "        self.indices = self.indices.persist()\n",
    "        \n",
    "    def load(self):\n",
    "        try:\n",
    "            self.dataset = xr.open_dataset(path = f'{self.path}/dataset.nc')\n",
    "        except Exception as e:\n",
    "             print(f'Primary dataset could not be opened: {e}')\n",
    "                \n",
    "        try:\n",
    "            self.mosaic = xr.open_dataset(path = f'{self.path}/mosaic.nc')\n",
    "        except Exception as e:\n",
    "            print(f'Moasaic dataset could not be opened: {e}')\n",
    "            \n",
    "        try:\n",
    "            self.indices = xr.open_dataset(path = f'{self.path}/indices.nc')\n",
    "        except Exception as e:\n",
    "            print(f'Indices dataset could not be opened: {e}')\n",
    "            \n",
    "    def save(self):\n",
    "        try:\n",
    "            self.dataset.to_netcdf(path = f'{self.path}/dataset.nc')\n",
    "        except Exception as e:\n",
    "            print(f'Primary dataset could not be saved: {e}')\n",
    "            \n",
    "        try:\n",
    "            self.mosaic.to_netcdf(path = f'{self.path}/mosaic.nc')\n",
    "        except Exception as e:\n",
    "            print(f'Moasaic dataset could not be saved: {e}')\n",
    "            \n",
    "        try:\n",
    "            self.indices.to_netcdf(path = f'{self.path}/indices.nc')\n",
    "        except Exception as e:\n",
    "            print(f'Indices dataset could not be saved: {e}')\n",
    "    \n",
    "    def show(self):\n",
    "        display(self.map)\n",
    "    \n",
    "    def create_mosaic(self, period = 'time.year'):\n",
    "        #creates yearly geomedian\n",
    "        self.mosaic = self.dataset.groupby(period).map(xr_geomedian)\n",
    "\n",
    "    def create_indices(self, indices = None):\n",
    "        # all these indices are no longer strictly necessary as we have settled on ENDISI but are nice to compare\n",
    "        if indices == None:\n",
    "            indices = [ 'NDBI', 'BAEI', 'BUI', 'NBI', 'MNDWI'] #TODO ENDISI not yet added to calculate_indices\n",
    "        \n",
    "        #define ENDISI\n",
    "        def ENDISI(dataset):\n",
    "            #Enhanced Normalized Difference Impervious Surfaces Index (ENDISI) from Chen et al. 2020\n",
    "            def Swir_diff(dataset):\n",
    "                return dataset.swir_1/dataset.swir_2\n",
    "\n",
    "            def Alpha(dataset):\n",
    "                return (2*(np.mean(dataset.blue)))/(np.mean(Swir_diff(dataset)) + np.mean(dataset.MNDWI)**2)\n",
    "\n",
    "            mndwi = dataset.MNDWI\n",
    "            swir_diff = Swir_diff(dataset)\n",
    "            alpha = Alpha(dataset)\n",
    "\n",
    "            return (dataset.blue - (alpha)*(swir_diff + mndwi**2))/(dataset.blue + (alpha)*(swir_diff + mndwi**2))\n",
    "\n",
    "        #adds indices to the dataset\n",
    "        for index in indices:\n",
    "            self.indices = calculate_indices(ds=self.mosaic, index = index, collection='s2')\n",
    "\n",
    "        #add ENDISI (hopefully will be adeed to the list later)\n",
    "        self.indices['ENDISI'] = ENDISI(self.indices)\n",
    "\n",
    "    def create_images(self, cmap = None):\n",
    "\n",
    "        if cmap == None: #default cmap\n",
    "            cmap = 'viridis'\n",
    "        #mask the water so its transparent on the map \n",
    "        ds_masked= self.mosaic.where(self.mosaic.MNDWI[1] <0) #make it \n",
    "\n",
    "        for index in indices + ['ENDISI']:\n",
    "            capture = ds_masked[index][1] #TODO generate full time dataset\n",
    "            #f = plt.figure(figsize= capture.shape)\n",
    "            plt.axis('off')\n",
    "            #im = f.figimage(X=capture, cmap= cmap ,resize=True)\n",
    "            #ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "            #plt.show()\n",
    "            filename =f'{self.path}/{index}.png'\n",
    "            plt.imsave(fname=filename, arr=capture, cmap=cmap, vmin = np.nanpercentile(capture, 1), vmax = np.nanpercentile(capture, 99))\n",
    "\n",
    "    def map_images(self):\n",
    "        \n",
    "        for index in indices + ['ENDISI']:\n",
    "            filename = f'{self.path}/{index}.png'\n",
    "            folium.raster_layers.ImageOverlay(\n",
    "                image=filename,\n",
    "                name = index,\n",
    "                opacity=.8,\n",
    "                bounds= [self.upper_left , self.lower_right],\n",
    "                show=False\n",
    "            ).add_to(self.map)\n",
    "            \n",
    "    \n",
    "    \n",
    "    def metrics(self, truth):\n",
    "        pass\n",
    "    \n",
    "    def analyze():\n",
    "        pass   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saly_analysis = Analysis(saly, dc) #lazy loads the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saly_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doesn't seem to work, think there is a problem calculating too much\n",
    "saly_analysis = saly_analysis.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saly_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saly_dataset = saly_analysis.dataset.compute() #seems to succeed sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saly_analysis.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saly_dataset.isel(time= slice(0,25))['red'].plot(x=\"x\", y=\"y\", col=\"time\", col_wrap=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
